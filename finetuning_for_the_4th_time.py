# -*- coding: utf-8 -*-
"""finetuning for the 4th time.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejolMq7auirtPJSBbtyPHuTQnBwUF13u
"""

from transformers import AutoModelForSeq2SeqLM , AutoTokenizer

model_name='google/flan-t5-small'
model=AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer=AutoTokenizer.from_pretrained(model_name)

from datasets import load_dataset,Dataset

data=load_dataset("databricks/databricks-dolly-15k")
data

data['train'][1]

def data_preprocessing(train):
  example=(train['instruction']+'\n'+train['context'])
  data_ids=tokenizer(example,padding='max_length',truncation=True,max_length=256,return_tensors='pt').input_ids[0]
  output_ids=tokenizer(train['response'],padding='max_length',truncation=True,max_length=256,return_tensors='pt').input_ids[0]

  return{'input_ids':data_ids,'labels':output_ids}

data_for_tokenization_sample=data['train'].shuffle(seed=42).select(range(2000))

tokenized_data=data_for_tokenization_sample.map(data_preprocessing)

from transformers import TrainingArguments,Trainer

training_args=TrainingArguments(
    output_dir='/outputs',
    per_device_train_batch_size=8,
    num_train_epochs=3,
    save_strategy='epoch',
    logging_steps=10,
    disable_tqdm=False,
    report_to='wandb',
    label_names=['labels']
)

from peft import LoraConfig,get_peft_model,TaskType
lora=LoraConfig(
    r=8,
    lora_alpha=16,lora_dropout=.1,
    bias='none',
    task_type=TaskType.SEQ_2_SEQ_LM

)

model1=get_peft_model(model,lora)

trainer1=Trainer(
    model=model1,
    args=training_args,
    train_dataset=tokenized_data
)

trainer1.train()

from peft import IA3Config

ia3=IA3Config(task_type=TaskType.SEQ_2_SEQ_LM,
              target_modules=['q','v','wi','wo'])
model2=get_peft_model(model,ia3)

trainer2=Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data
)

trainer2.train()

